{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Text Classifier\n",
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3\n",
      "/usr/local/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import copy\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from tqdm import tqdm # progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tokenize = lambda x: lemma.lemmatize(re.sub(r'<.*?>|[^\\w\\s]|\\d+', '', x)).split()\n",
    "    \n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True,\n",
    "                       include_lengths=True, batch_first=True, dtype=torch.long) #fix_length=200,\n",
    "    LABEL = data.LabelField(dtype=torch.float, sequential=False)\n",
    "\n",
    "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    \n",
    "    TEXT.build_vocab(train, max_size=25000, vectors=GloVe(name='6B', dim=300)) # Glove Embedding\n",
    "    LABEL.build_vocab(train)\n",
    "    word_emb = TEXT.vocab.vectors\n",
    "    \n",
    "    train, valid = train.split()\n",
    "    train_data, valid_data, test_data = data.BucketIterator.splits((train, valid, test),\n",
    "                                                                   batch_size=64, repeat=False, shuffle=True)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "    print (\"\\nSize of train set: {} \\nSize of validation set: {} \\nSize of test set: {}\".format(len(train_data.dataset), len(valid_data.dataset), len(test_data.dataset)))\n",
    "    print(LABEL.vocab.freqs.most_common(2))\n",
    "\n",
    "    return TEXT, word_emb, train_data, valid_data, test_data, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 25002\n",
      "Vector size of Text Vocabulary:  torch.Size([25002, 300])\n",
      "Label Length: 2\n",
      "\n",
      "Size of train set: 17500 \n",
      "Size of validation set: 7500 \n",
      "Size of test set: 25000\n",
      "[('pos', 12500), ('neg', 12500)]\n"
     ]
    }
   ],
   "source": [
    "TEXT, word_emb, train_data, valid_data, test_data, vocab_size = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 823])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in train_data:\n",
    "    b = a.text\n",
    "    break\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    10,     17,     90,     69,     51,    664,     29,   1127,\n",
       "             21,      6],\n",
       "        [    10,   3405,   3544,   2758,     19,     43,   5118,    394,\n",
       "              4,    493],\n",
       "        [    11,   1016,     10,     19,    527,     20,      4,   1708,\n",
       "            309,   2653],\n",
       "        [    11,    208,     10,     17,     20,    746,   1098,    729,\n",
       "              9,   2063],\n",
       "        [   796,   4160,    283,   1795,      0,   8463,      4,   3052,\n",
       "             16,      4],\n",
       "        [     2,     19,   2143,   2557,     20,      4,   9969,    651,\n",
       "           1807,     14],\n",
       "        [   976,      0,     11,    976,  18726,      2,   7157,      3,\n",
       "              2,   1798],\n",
       "        [    10,     19,      7,    464,      3,     11,     57,    365,\n",
       "              6,    508],\n",
       "        [    11,    208,     10,   1640,     19,     52,      9,     13,\n",
       "              8,   9446],\n",
       "        [  6025,      5,      2,    444,   3080,     64,   8255,     45,\n",
       "             22,    208],\n",
       "        [    11,    208,     10,     19,     30,      2,  11242,     19,\n",
       "           1352,      3],\n",
       "        [  5494,     11,     57,     37,      6,    213,     44,     12,\n",
       "             79,     35],\n",
       "        [    11,    402,      9,     11,     62,    116,    623,     89,\n",
       "            102,     12],\n",
       "        [    11,     66,    281,      3,   4198,     10,      4,    163,\n",
       "            147,    141],\n",
       "        [  1774,   5939,   8080,    839,     33,  13092,    153,      4,\n",
       "            169,      5],\n",
       "        [     2,   3703,   8358,   3765,    871,   1174,   2331,   3320,\n",
       "              3,    722],\n",
       "        [     2,    272,     11,   1368,      6,    191,     10,     17,\n",
       "              4,    560],\n",
       "        [    22,     68,    358,     34,   1042,    246,  21495,      7,\n",
       "              4,    419],\n",
       "        [    10,      7,      4,    706,    220,     15,      2,   4138,\n",
       "             20,    220],\n",
       "        [   525,     11,     13,    239,   1484,     15,      2,   1773,\n",
       "              2,   2274],\n",
       "        [  1958,   1340,      3,   1967,   4729,   9918,      6,     82,\n",
       "             46,      5],\n",
       "        [   139,     11,     62,    115,     42,     10,   3067,    347,\n",
       "             93,     13],\n",
       "        [    11,   9735,     10,     17,     21,     61,     78,     11,\n",
       "            237,      4],\n",
       "        [   380,      2,     87,   4738,      3,   3560,      5,     31,\n",
       "           1324,     93],\n",
       "        [    10,     17,   2328,    156,    152,     40,   2554,    100,\n",
       "            306,      9],\n",
       "        [   410,     28,      5,      2,    240,     91,      5,      2,\n",
       "            321,      2],\n",
       "        [     2,   1218,      5,   2092,     93,     37,     10,     28,\n",
       "              7,     12],\n",
       "        [    11,     62,    446,      6,     37,      2,  12929,    270,\n",
       "           1699,     63],\n",
       "        [    10,     17,     13,     96,  12488,      4,    169,      5,\n",
       "              2,    712],\n",
       "        [    85,     34,    183,   1555,   2727,      6,    294,      8,\n",
       "             10,     17],\n",
       "        [    11,    237,    737,     70,    105,    712,   1033,     50,\n",
       "           1151,     16],\n",
       "        [     2,    112,     66,     46,   4993,   1285,   1240,    199,\n",
       "              2,   1131],\n",
       "        [    10,  21536,    244,   3209,    205,     14,     49,     14,\n",
       "              2,    196],\n",
       "        [     2,    830,      5,     10,  12252,    750,     13,     62,\n",
       "             49,      3],\n",
       "        [   939,      6,      2,    408,    187,     13,    805,  20398,\n",
       "             15,      2],\n",
       "        [  4818,     15,    460,    149,     11,    841,     72,     50,\n",
       "             36,      0],\n",
       "        [    11,     86,    281,     10,     17,     20,     29,    771,\n",
       "              8,      3],\n",
       "        [     6,    103,     10,     19,     36,    366,      6,   1331,\n",
       "            202,  10725],\n",
       "        [   488,     78,      5,    170,    712,   6358,      6,      0,\n",
       "           2519,     10],\n",
       "        [    10,     19,     90,    303,   3511,      4,    329,   1559,\n",
       "              0,      7],\n",
       "        [    11,    208,     10,    654,   1522,     19,   4370,     29,\n",
       "            731,   6152],\n",
       "        [  4850,  23020,      7,     58,     86,   2134,      8,    946,\n",
       "             54,    150],\n",
       "        [    28,   3406,    133,     10,    427,     13,     90,     30,\n",
       "             31,      2],\n",
       "        [     2,   1311,      5,     10,     19,     25,   1048,      4,\n",
       "            693,    113],\n",
       "        [   618,   3494,   1642,     14,     11,    281,      2,     86,\n",
       "            355,      5],\n",
       "        [    10,  19148,    963,   2420,    204,    973,   1067,      8,\n",
       "              2,    244],\n",
       "        [     2,     81,   1595,      7,     29,    125,     62,   2066,\n",
       "             11,    492],\n",
       "        [  1642,   4837,     31,    456,   3768,      2,   1489,    667,\n",
       "             77,      4],\n",
       "        [    11,     25,    429,     10,     17,    234,     11,     86,\n",
       "            208,      9],\n",
       "        [    37,    105,     79,    131,     11,    616,     44,   1438,\n",
       "             58,   4977],\n",
       "        [    85,     68,      4,     17,     25,  11441,  14931,      3,\n",
       "            125,   2768],\n",
       "        [    48,     23,    428,    180,    356,     15,     10,     17,\n",
       "           4839,    670],\n",
       "        [    10,      7,     28,      5,      2,    240,    180,      6,\n",
       "            121,    209],\n",
       "        [    29,     42,     60,     12,   6414,   5323,      7,    615,\n",
       "             20,    292],\n",
       "        [    11,    125,     89,    117,    133,     11,    877,    523,\n",
       "              6,    836],\n",
       "        [   168,     17,     36,      2,   6749,      7,   1876,     18,\n",
       "             45,     22],\n",
       "        [    11,     89,    117,     47,   3926,      0,     13,    516,\n",
       "              6,   1656],\n",
       "        [    11,     86,    151,    177,      6,    103,     10,     19,\n",
       "             16,      2],\n",
       "        [    11,     13,     62,   4867,     85,     72,      4,     19,\n",
       "            178,    124],\n",
       "        [    74,     47,     68,     22,    129,     42,      4,   2146,\n",
       "              0,      0],\n",
       "        [    11,     25,     41,    107,     10,     19,      3,   2762,\n",
       "              8,    115],\n",
       "        [  2670,     11,    183,   2018,      2,    506,    111,     11,\n",
       "            183,   2018],\n",
       "        [     4,   1918,      6,      2,      0,   6410,      8,      0,\n",
       "          16287,   4198],\n",
       "        [    11,    534,    171,      8,      4,    110,   4479,    359,\n",
       "              5,     10]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25002, 300])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 328121), ('and', 161575), ('a', 161309), ('of', 145166), ('to', 134822), ('is', 106799), ('in', 92187), ('it', 76313), ('this', 73186), ('i', 72475), ('that', 69198), ('was', 47988), ('as', 46058), ('with', 43724), ('for', 43701), ('movie', 41826), ('but', 40999), ('film', 37487), ('on', 33340), ('not', 30012)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'this', 'i', 'that', 'was', 'as', 'with', 'for', 'movie', 'but', 'film']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, seq_len, emb_dim, hidden_dim, output_dim, embedding, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.seq_len = seq_len # vocabulary dim\n",
    "        self.emb_dim = emb_dim # glove dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = embedding # glove embedding\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initalize look-up table\n",
    "        self.word_emb = torch.nn.Embedding(seq_len, emb_dim, padding_idx=1)\n",
    "        # connect look-up table to glove embedding\n",
    "        self.word_emb.weight = torch.nn.Parameter(embedding, requires_grad=False)\n",
    "        \n",
    "        # Layers: one LSTM, one Fully-connected\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim, dropout=0.5)\n",
    "        #self.lstm = nn.LSTM(seg_len, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        #print(\"testing\")\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        #self.init_weights()\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        # input_emb (num_seq, batch_size, emb_dim)\n",
    "        #print(\"X: \", x.shape)\n",
    "        emb_input = self.word_emb(x).permute(1, 0, 2)  \n",
    "        #print(\"emb_input: \", emb_input.shape)\n",
    "        hidden_state = (torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        cell_state = (torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        #print(\"hidden_state: \", hidden_state.shape)\n",
    "        output, (hidden_out, cell_out) = self.lstm(emb_input, (hidden_state, cell_state))\n",
    "        #print(\"out: \", out.shape)\n",
    "        #print(\"hidden_out: \", hidden_out.shape)\n",
    "        output = self.dropout(output)\n",
    "        y_pred = (self.fc(hidden_out[-1]))\n",
    "        #print(\"y_pred: \", y_pred)\n",
    "        return y_pred\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.LSTM(embedded)\n",
    "        return outputs, hidden\n",
    "    \"\"\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200 #vocab_size\n",
    "emb_dim = 300\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "embedding = word_emb\n",
    "batch_size = 64\n",
    "\n",
    "model = LSTM(seq_len, emb_dim, hidden_dim, output_dim, embedding, batch_size)\n",
    "init_lstm_weights = copy.deepcopy(model.state_dict())\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "#criterion = torch.nn.CrossEntropyLoss(ignore_index=TEXT.vocab.stoi['<pad>'])\n",
    "optimizer = torch.optim.SGD(filter(lambda param: param.requires_grad, model.parameters()), lr=0.01)\n",
    "#optimizer = torch.optim.Adam(filter(lambda param: param.requires_grad, model.parameters()) ) #,lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x1a306c52b0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def train(model, data_iter, epoch_size, criterion, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    #for epoch in tqdm(range(epoch_size)):\n",
    "    for epoch in range(epoch_size):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad() \n",
    "            loss = 0\n",
    "            correct = 0\n",
    "            #print(\"\\n batch {}: \".format(i))\n",
    "            batch_size = len(batch.text[0])\n",
    "            pred = model(batch.text[0], batch_size)\n",
    "            #pred = torch.max(pred, 1)[1].view(-1)\n",
    "            #print(\"\\tprediction: \", pred)\n",
    "            pred = torch.max(pred, 1)[1]\n",
    "            #print(\"\\tprediction: \", pred)\n",
    "            #print(\"\\ttarget: \", batch.label.view(-1))\n",
    "            loss = Variable(criterion(pred.float(), batch.label.float()), requires_grad = True)\n",
    "            epoch_loss.append(loss)\n",
    "            #loss = (criterion(pred.float(), batch.label.float()))\n",
    "\n",
    "            correct = ((pred.float() == batch.label).sum()).numpy()\n",
    "            #correct = (pred == batch.label.type(torch.LongTensor)).sum().data\n",
    "            #print(\"Correct: \", correct)\n",
    "            #print(correct)\n",
    "            acc = correct/pred.shape[0]\n",
    "            epoch_acc.append(acc)\n",
    "            #print(acc)\n",
    "     \n",
    "            loss.backward()\n",
    "            clip_gradient(model, 0.25)\n",
    "            optimizer.step()\n",
    "\n",
    "            #print(\"------Batch {}/{}, Batch Loss: {:.4f}, Accuracy: {:.4f}\".format(i+1,len(data_iter), loss, acc))\n",
    "        total_loss.append((sum(epoch_loss)/len(data_iter)))\n",
    "        total_acc.append((sum(total_acc)/len(data_iter)))\n",
    "        print(\"****** Epoch {} Loss: {}, Epoch {} Acc: {}\".format(epoch, (sum(epoch_loss)/len(data_iter)),\n",
    "                                                                  epoch, (sum(epoch_acc)/len(data_iter))))          \n",
    "    return total_loss, total_acc\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Epoch 0 Loss: 13.894750595092773, Epoch 0 Acc: 0.49713242961418147\n",
      "****** Epoch 1 Loss: 13.893407821655273, Epoch 1 Acc: 0.4971813086548487\n",
      "****** Epoch 2 Loss: 13.901503562927246, Epoch 2 Acc: 0.49688803441084467\n",
      "****** Epoch 3 Loss: 13.87471866607666, Epoch 3 Acc: 0.49785746871741404\n",
      "****** Epoch 4 Loss: 13.901054382324219, Epoch 4 Acc: 0.49690432742440044\n",
      "****** Epoch 5 Loss: 13.88597297668457, Epoch 5 Acc: 0.4974501433785193\n",
      "****** Epoch 6 Loss: 13.887322425842285, Epoch 6 Acc: 0.497401264337852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-fcb608decb72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-295d0cef8679>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_iter, epoch_size, criterion, optimizer)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#print(\"\\n batch {}: \".format(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m#pred = torch.max(pred, 1)[1].view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#print(\"\\tprediction: \", pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-b82dfd29ebeb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(\"out: \", out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(\"hidden_out: \", hidden_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#print(\"y_pred: \", y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = train(model, train_data, 20, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHyperparameter optimization\\na) learning rates\\nb) different numbers of hidden layers\\nc) different sizes of hidden layers\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hyperparameter optimization\n",
    "a) learning rates\n",
    "b) different numbers of hidden layers\n",
    "c) different sizes of hidden layers\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
