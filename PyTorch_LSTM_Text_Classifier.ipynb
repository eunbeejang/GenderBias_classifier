{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Text Classifier\n",
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eunbeejang/anaconda3\n",
      "/usr/local/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.prefix)\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional\n",
    "import copy\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "from tqdm import tqdm # progress bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tokenize = lambda x: lemma.lemmatize(re.sub(r'<.*?>|[^\\w\\s]|\\d+', '', x)).split()\n",
    "    \n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True,\n",
    "                       include_lengths=True, batch_first=True, dtype=torch.long) #fix_length=200,\n",
    "    LABEL = data.LabelField(batch_first=True, sequential=False)\n",
    "\n",
    "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    \n",
    "    TEXT.build_vocab(train, max_size=25000, vectors=GloVe(name='6B', dim=300)) # Glove Embedding\n",
    "    LABEL.build_vocab(train)\n",
    "    word_emb = TEXT.vocab.vectors\n",
    "    \n",
    "    train, valid = train.split()\n",
    "    train_data, valid_data, test_data = data.BucketIterator.splits((train, valid, test),\n",
    "                                                                   batch_size=64, repeat=False, shuffle=True)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "    print (\"\\nSize of train set: {} \\nSize of validation set: {} \\nSize of test set: {}\".format(len(train_data.dataset), len(valid_data.dataset), len(test_data.dataset)))\n",
    "    print(LABEL.vocab.freqs.most_common(2))\n",
    "\n",
    "    return TEXT, word_emb, train_data, valid_data, test_data, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 25002\n",
      "Vector size of Text Vocabulary:  torch.Size([25002, 300])\n",
      "Label Length: 2\n",
      "\n",
      "Size of train set: 17500 \n",
      "Size of validation set: 7500 \n",
      "Size of test set: 25000\n",
      "[('pos', 12500), ('neg', 12500)]\n"
     ]
    }
   ],
   "source": [
    "TEXT, word_emb, train_data, valid_data, test_data, vocab_size = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 994])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for a in train_data:\n",
    "    b = a.text\n",
    "    break\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    16,      2,   2274,    230,     40,     38,     11,     13,\n",
       "            146,      9],\n",
       "        [     2,    231,    116,      4,    169,      5,    432,   1570,\n",
       "             16,      4],\n",
       "        [    33,   6848,  22956,  14019,    251,     32,      4,    176,\n",
       "              3,    805],\n",
       "        [  1250,  22470,    108,   1810,    647,      6,      2,    245,\n",
       "             36,      2],\n",
       "        [   484,    130,     85,    476,   6536,     66,   2762,     14,\n",
       "             32,     10],\n",
       "        [     2,    426,     15,      2,      0,      7,   7548,     55,\n",
       "             32,      2],\n",
       "        [    10,     17,     13,      4,    517,   1096,      9,     13,\n",
       "            196,   4465],\n",
       "        [  1760,   5437,      7,      4,    293,     60,   3360,   1253,\n",
       "           6528,   1980],\n",
       "        [   208,     10,     17,     52,      9,    364,     44,      8,\n",
       "            304,      4],\n",
       "        [   386,     44,      5,      4,    618,      3,     12,      7,\n",
       "            106,   2087],\n",
       "        [    11,    595,    319,    816,    155,     11,    103,      4,\n",
       "             17,    468],\n",
       "        [     0,     10,     17,     13,    179,     72,   3452,      3,\n",
       "             29,    681],\n",
       "        [  2078,      2,     61,     80,    287,     35,     57,     26,\n",
       "            322,   1258],\n",
       "        [    10,    392,     19,     43,     29,   1497,      4,    710,\n",
       "            112,      3],\n",
       "        [     9,      7,    264,      6,   1549,   6301,      8,    685,\n",
       "              9,      7],\n",
       "        [    10,      7,      4,    123,     70,    853,   1223,     17,\n",
       "              3,     29],\n",
       "        [    10,      7,      2,    333,    673,   4044,     19,      6,\n",
       "           7570,      2],\n",
       "        [    82,     22,     37,    835,     55,     92,    760,    241,\n",
       "             36,      0],\n",
       "        [    66,   2753,   1432,     73,   1466,    301,     27,    236,\n",
       "             57,     25],\n",
       "        [    10,     19,   1683,     69,      5,   3172,    876,   1174,\n",
       "          21748,   5049],\n",
       "        [    74,   1814,      5,  24355,      7,    681,     33,   1050,\n",
       "             17,      2],\n",
       "        [ 23962,   2828,      0,      0,      2,  21064,   2828,      0,\n",
       "            283,   2157],\n",
       "        [    18,     85,     68,     22,    835,      6,      0,      4,\n",
       "           2311,    761],\n",
       "        [    10,     19,      7,      4,   9756,    558,      6,   2269,\n",
       "              8,      2],\n",
       "        [  2331,  12504,      7,      0,   6616,      4,   3203,   4757,\n",
       "             35,    291],\n",
       "        [    74,     11,     89,    117,    113,      6,    870,    518,\n",
       "             10,     13],\n",
       "        [    98,     17,     12,   2098,      2,  11074,   1789,    665,\n",
       "             14,      2],\n",
       "        [    11,   1896,     10,     19,    100,    869,      2,    270,\n",
       "             12,   1580],\n",
       "        [     8,      4,    770,    693,      2,   1864,    130,    739,\n",
       "             98,   4621],\n",
       "        [   143,     21,    243,     11,    368,    113,     31,    128,\n",
       "          11985,      0],\n",
       "        [   484,    209,     20,    810,      6,     25,      4,    110,\n",
       "            250,     52],\n",
       "        [    74,     46,     79,     57,    129,     12,     10,    806,\n",
       "             17,      0],\n",
       "        [    34,    138,     25,    740,     10,     17,   1908,    361,\n",
       "             12,    864],\n",
       "        [  2100,      0,      8,      2,      0,     25,     56,    313,\n",
       "             47,     11],\n",
       "        [    10,    569,    746,     15,    607,   1460,      7,    207,\n",
       "             20,    813],\n",
       "        [    10,    195,     17,      7,     62,      0,      7,     45,\n",
       "             10,      7],\n",
       "        [    10,      7,     33,    312,     17,     14,      4,   2240,\n",
       "             35,   1957],\n",
       "        [    10,     17,     13,    506,    316,    267,      9,     13,\n",
       "             38,     81],\n",
       "        [     2,      0,     12,      0,     20,     10,     19,     23,\n",
       "             74,    604],\n",
       "        [  1133,    353,     19,     15,   3236,      0,    323,   1037,\n",
       "            615,     16],\n",
       "        [    11,     13,    400,      6,    720,     10,     16,      4,\n",
       "           3741,      3],\n",
       "        [    10,    192,    153,     26,      2,    240,     17,     12,\n",
       "             11,     25],\n",
       "        [    10,     17,  20004,     15,     69,     20,    104,   2162,\n",
       "             14,      4],\n",
       "        [   651,    832,   4566,   4055,   7180,   1783,   4107,  22258,\n",
       "              3,     31],\n",
       "        [     0,   1253,     20,      2,    356,   6789,      0,    574,\n",
       "              5,  15147],\n",
       "        [    10,    144,    175,      0,      2,   6061,      5,  13172,\n",
       "             18,      9],\n",
       "        [   280,    713,      4,     60,     48,     13,      4,    174,\n",
       "             32,      2],\n",
       "        [ 17833,      0,  16444,    235,     59,   1136,     39,     33,\n",
       "          14023,   1372],\n",
       "        [   285,    138,    358,   7082,   7755,     12,     39,    611,\n",
       "             14,      4],\n",
       "        [   710,    112,    580,    405,   6742,  19156,    346,     18,\n",
       "            651,   5213],\n",
       "        [   143,     21,      4,    328,      5,   1820,   3142,      8,\n",
       "            185,     11],\n",
       "        [     0,     43,    242,      6,   1848,    407,    168,    675,\n",
       "             17,     36],\n",
       "        [    38,   3705,    309,     13,    400,      6,     26,      4,\n",
       "            195,      3],\n",
       "        [    11,   1799,    492,     10,    378,      4,    711,      5,\n",
       "            104,     11],\n",
       "        [   136,     11,    995,    492,   4775,      0,      4,   1077,\n",
       "            720,     11],\n",
       "        [  3604,   1161,   3340,      2,    238,      5,   1098,   1276,\n",
       "             12,      7],\n",
       "        [   200,    763,      6,     37,     10,     19,     62,      8,\n",
       "            146,      9],\n",
       "        [    10,      7,    210,    756,   2150,    199,     47,    154,\n",
       "             10,     19],\n",
       "        [    14,   8458,     14,     11,     68,     11,   5879,    256,\n",
       "             10,     17],\n",
       "        [    11,    177,      6,  14656,      4,    163,    180,     11,\n",
       "            237,     21],\n",
       "        [    10,    812,      7,     42,     14,     81,     14,     34,\n",
       "            209,     11],\n",
       "        [ 15652,   1741,   9439,    386,    284,      8,     10,     17,\n",
       "            227,     25],\n",
       "        [    11,    468,     11,     61,     25,    523,      6,   1737,\n",
       "             16,      2],\n",
       "        [ 12847,   7636,      3,   3211,   9787,    329,      8,     10,\n",
       "            690,    607]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 328121), ('and', 161575), ('a', 161309), ('of', 145166), ('to', 134822), ('is', 106799), ('in', 92187), ('it', 76313), ('this', 73186), ('i', 72475), ('that', 69198), ('was', 47988), ('as', 46058), ('with', 43724), ('for', 43701), ('movie', 41826), ('but', 40999), ('film', 37487), ('on', 33340), ('not', 30012)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'this', 'i', 'that', 'was', 'as', 'with', 'for', 'movie', 'but', 'film']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, seq_len, emb_dim, hidden_dim, output_dim, embedding, batch_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.seq_len = seq_len \n",
    "        self.emb_dim = emb_dim # glove dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = embedding # glove embedding\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initalize look-up table and assign weight\n",
    "        self.word_emb = torch.nn.Embedding(25002, emb_dim)\n",
    "        #self.word_emb.weight = torch.nn.Parameter(embedding)\n",
    "        # Layers: one LSTM, one Fully-connected\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, batch):\n",
    "        x = self.word_emb(x).permute(1, 0, 2)\n",
    "        h_0 = self._init_state(batch_size=batch)\n",
    "        out, (hidden_out, cell_out) = self.lstm(x, h_0)\n",
    "        self.dropout(hidden_out)\n",
    "        y_pred = self.fc(hidden_out[-1])\n",
    "        return y_pred\n",
    "\n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        #print(\"WEIGHT IS: \", weight)\n",
    "        return (\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "            weight.new(self.num_layers, batch_size, self.hidden_dim).zero_()\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (word_emb): Embedding(25002, 300)\n",
      "  (lstm): LSTM(300, 256)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "emb_dim = 300\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "embedding = word_emb\n",
    "lr = 0.001\n",
    "max_grad_norm = 5\n",
    "\n",
    "\n",
    "model = LSTM(seq_len, emb_dim, hidden_dim, output_dim, embedding, batch_size)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(filter(lambda param: param.requires_grad, model.parameters()), lr=lr ) #,lr=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "        \n",
    "def train(model, data_iter, epoch_size, optimizer):\n",
    "    #init_lstm_weights = copy.deepcopy(model.state_dict())\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "#    for epoch in tqdm(range(epoch_size)):\n",
    "    for epoch in range(epoch_size):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        \n",
    "        for i, batch in enumerate(data_iter):\n",
    "            batch_size = len(batch.text[0])\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            #loss = 0\n",
    "            #correct = 0\n",
    "            pred = model(batch.text[0],batch_size)\n",
    "            loss = functional.cross_entropy(pred, batch.label, size_average=False)            \n",
    "            correct = ((torch.max(pred, 1)[1] == batch.label)).sum().numpy()\n",
    "            acc = correct/pred.shape[0]\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "     \n",
    "            loss.backward() # calculate the gradient\n",
    "        \n",
    "            #clip_gradient(model, 0.25) # limit the norm\n",
    "            # Clip to the gradient to avoid exploding gradient.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step() # update param\n",
    "\n",
    "            print(\"------Batch {}/{}, Batch Loss: {:.4f}, Accuracy: {:.4f}\".format(i+1,len(data_iter), loss, acc))\n",
    "        \n",
    "        total_loss.append((sum(epoch_loss)/len(data_iter)))\n",
    "        total_acc.append((sum(total_acc)/len(data_iter)))\n",
    "        print(\"****** Epoch {} Loss: {}, Epoch {} Acc: {}\".format(epoch, (sum(epoch_loss)/len(data_iter)),\n",
    "                                                                  epoch, (sum(epoch_acc)/len(data_iter))))          \n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    model.eval()\n",
    "\n",
    "    for i, batch in enumerate(val_iter):\n",
    "        batch_size = len(batch.text[0])\n",
    "        pred = model(batch.text[0],batch_size)\n",
    "        loss = functional.cross_entropy(pred, batch.label, size_average=False)            \n",
    "        correct = ((torch.max(pred, 1)[1] == batch.label)).sum().numpy()\n",
    "        acc = correct/pred.shape[0]\n",
    "        total_loss.append(loss.item())\n",
    "        total_acc.append(acc)\n",
    "        print(\"------Batch {}/{}, Batch Loss: {:.4f}, Accuracy: {:.4f}\".format(i+1,len(val_iter), loss, acc))\n",
    "    print(\"Average Loss: \", (sum(total_loss) / len(val_iter))) \n",
    "    print(\"Average Acc: \", (sum(total_acc) / len(val_iter))) \n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Batch 1/274, Batch Loss: 47.1639, Accuracy: 0.4375\n",
      "------Batch 2/274, Batch Loss: 39.6268, Accuracy: 0.7188\n",
      "------Batch 3/274, Batch Loss: 54.1900, Accuracy: 0.4844\n",
      "------Batch 4/274, Batch Loss: 45.4246, Accuracy: 0.5312\n",
      "------Batch 5/274, Batch Loss: 44.2980, Accuracy: 0.5312\n",
      "------Batch 6/274, Batch Loss: 47.2682, Accuracy: 0.4844\n",
      "------Batch 7/274, Batch Loss: 56.0319, Accuracy: 0.3281\n",
      "------Batch 8/274, Batch Loss: 44.3350, Accuracy: 0.5781\n",
      "------Batch 9/274, Batch Loss: 48.3397, Accuracy: 0.4688\n",
      "------Batch 10/274, Batch Loss: 43.0490, Accuracy: 0.5938\n",
      "------Batch 11/274, Batch Loss: 46.4452, Accuracy: 0.4219\n",
      "------Batch 12/274, Batch Loss: 43.9218, Accuracy: 0.5625\n",
      "------Batch 13/274, Batch Loss: 44.6400, Accuracy: 0.4531\n",
      "------Batch 14/274, Batch Loss: 44.5363, Accuracy: 0.4531\n",
      "------Batch 15/274, Batch Loss: 44.6775, Accuracy: 0.4531\n",
      "------Batch 16/274, Batch Loss: 43.8275, Accuracy: 0.6094\n",
      "------Batch 17/274, Batch Loss: 44.3949, Accuracy: 0.5000\n",
      "------Batch 18/274, Batch Loss: 44.8699, Accuracy: 0.4219\n",
      "------Batch 19/274, Batch Loss: 44.2449, Accuracy: 0.5156\n",
      "------Batch 20/274, Batch Loss: 44.3362, Accuracy: 0.5938\n",
      "------Batch 21/274, Batch Loss: 44.2241, Accuracy: 0.5469\n",
      "------Batch 22/274, Batch Loss: 44.6022, Accuracy: 0.4375\n",
      "------Batch 23/274, Batch Loss: 44.1443, Accuracy: 0.5469\n",
      "------Batch 24/274, Batch Loss: 44.2001, Accuracy: 0.5469\n",
      "------Batch 25/274, Batch Loss: 44.7116, Accuracy: 0.4688\n",
      "------Batch 26/274, Batch Loss: 44.1263, Accuracy: 0.5469\n",
      "------Batch 27/274, Batch Loss: 45.1865, Accuracy: 0.4375\n",
      "------Batch 28/274, Batch Loss: 44.1434, Accuracy: 0.5312\n",
      "------Batch 29/274, Batch Loss: 45.1386, Accuracy: 0.4219\n",
      "------Batch 30/274, Batch Loss: 44.3546, Accuracy: 0.5000\n",
      "------Batch 31/274, Batch Loss: 44.1979, Accuracy: 0.5156\n",
      "------Batch 32/274, Batch Loss: 44.5142, Accuracy: 0.5312\n",
      "------Batch 33/274, Batch Loss: 44.6487, Accuracy: 0.4688\n",
      "------Batch 34/274, Batch Loss: 44.4293, Accuracy: 0.5156\n",
      "------Batch 35/274, Batch Loss: 44.1078, Accuracy: 0.5469\n",
      "------Batch 36/274, Batch Loss: 45.1041, Accuracy: 0.4531\n",
      "------Batch 37/274, Batch Loss: 43.6899, Accuracy: 0.5938\n",
      "------Batch 38/274, Batch Loss: 44.2820, Accuracy: 0.5312\n",
      "------Batch 39/274, Batch Loss: 43.0428, Accuracy: 0.6406\n",
      "------Batch 40/274, Batch Loss: 43.8012, Accuracy: 0.5781\n",
      "------Batch 41/274, Batch Loss: 44.7659, Accuracy: 0.4844\n",
      "------Batch 42/274, Batch Loss: 43.5581, Accuracy: 0.5938\n",
      "------Batch 43/274, Batch Loss: 47.0980, Accuracy: 0.3438\n",
      "------Batch 44/274, Batch Loss: 43.8136, Accuracy: 0.5625\n",
      "------Batch 45/274, Batch Loss: 45.9574, Accuracy: 0.4375\n",
      "------Batch 46/274, Batch Loss: 45.8667, Accuracy: 0.4375\n",
      "------Batch 47/274, Batch Loss: 44.5294, Accuracy: 0.5312\n",
      "------Batch 48/274, Batch Loss: 44.3959, Accuracy: 0.5469\n",
      "------Batch 49/274, Batch Loss: 45.7085, Accuracy: 0.3906\n",
      "------Batch 50/274, Batch Loss: 44.1827, Accuracy: 0.5156\n",
      "------Batch 51/274, Batch Loss: 44.1075, Accuracy: 0.5156\n",
      "------Batch 52/274, Batch Loss: 43.8691, Accuracy: 0.6250\n",
      "------Batch 53/274, Batch Loss: 44.9735, Accuracy: 0.4531\n",
      "------Batch 54/274, Batch Loss: 44.3213, Accuracy: 0.5469\n",
      "------Batch 55/274, Batch Loss: 44.0549, Accuracy: 0.5781\n",
      "------Batch 56/274, Batch Loss: 43.9307, Accuracy: 0.5469\n",
      "------Batch 57/274, Batch Loss: 46.1534, Accuracy: 0.3438\n",
      "------Batch 58/274, Batch Loss: 44.4180, Accuracy: 0.5156\n",
      "------Batch 59/274, Batch Loss: 44.5035, Accuracy: 0.5156\n",
      "------Batch 60/274, Batch Loss: 43.9089, Accuracy: 0.5781\n",
      "------Batch 61/274, Batch Loss: 45.4358, Accuracy: 0.4375\n",
      "------Batch 62/274, Batch Loss: 45.8623, Accuracy: 0.4219\n",
      "------Batch 63/274, Batch Loss: 44.5577, Accuracy: 0.5000\n",
      "------Batch 64/274, Batch Loss: 44.4418, Accuracy: 0.4531\n",
      "------Batch 65/274, Batch Loss: 43.8844, Accuracy: 0.5625\n",
      "------Batch 66/274, Batch Loss: 44.0266, Accuracy: 0.5938\n",
      "------Batch 67/274, Batch Loss: 44.1107, Accuracy: 0.5000\n",
      "------Batch 68/274, Batch Loss: 44.2131, Accuracy: 0.5000\n",
      "------Batch 69/274, Batch Loss: 44.2510, Accuracy: 0.4844\n",
      "------Batch 70/274, Batch Loss: 44.4010, Accuracy: 0.4688\n",
      "------Batch 71/274, Batch Loss: 44.4849, Accuracy: 0.4531\n",
      "------Batch 72/274, Batch Loss: 43.8612, Accuracy: 0.5938\n",
      "------Batch 73/274, Batch Loss: 44.4552, Accuracy: 0.4531\n",
      "------Batch 74/274, Batch Loss: 44.5754, Accuracy: 0.4688\n",
      "------Batch 75/274, Batch Loss: 44.1072, Accuracy: 0.5625\n",
      "------Batch 76/274, Batch Loss: 45.1964, Accuracy: 0.5156\n",
      "------Batch 77/274, Batch Loss: 44.7575, Accuracy: 0.3750\n",
      "------Batch 78/274, Batch Loss: 43.9186, Accuracy: 0.5000\n",
      "------Batch 79/274, Batch Loss: 44.2630, Accuracy: 0.5469\n",
      "------Batch 80/274, Batch Loss: 43.8377, Accuracy: 0.5469\n",
      "------Batch 81/274, Batch Loss: 43.7932, Accuracy: 0.5469\n",
      "------Batch 82/274, Batch Loss: 45.0674, Accuracy: 0.5469\n",
      "------Batch 83/274, Batch Loss: 44.6722, Accuracy: 0.4531\n",
      "------Batch 84/274, Batch Loss: 44.5559, Accuracy: 0.4844\n",
      "------Batch 85/274, Batch Loss: 44.3690, Accuracy: 0.5156\n",
      "------Batch 86/274, Batch Loss: 45.1436, Accuracy: 0.5781\n",
      "------Batch 87/274, Batch Loss: 43.9172, Accuracy: 0.5469\n",
      "------Batch 88/274, Batch Loss: 45.4534, Accuracy: 0.4375\n",
      "------Batch 89/274, Batch Loss: 43.2794, Accuracy: 0.6562\n",
      "------Batch 90/274, Batch Loss: 44.2312, Accuracy: 0.5000\n",
      "------Batch 91/274, Batch Loss: 45.1836, Accuracy: 0.4375\n",
      "------Batch 92/274, Batch Loss: 45.3524, Accuracy: 0.4375\n",
      "------Batch 93/274, Batch Loss: 44.7624, Accuracy: 0.4688\n",
      "------Batch 94/274, Batch Loss: 44.0950, Accuracy: 0.5312\n",
      "------Batch 95/274, Batch Loss: 43.8410, Accuracy: 0.5938\n",
      "------Batch 96/274, Batch Loss: 44.4113, Accuracy: 0.5000\n",
      "------Batch 97/274, Batch Loss: 43.8203, Accuracy: 0.4219\n",
      "------Batch 98/274, Batch Loss: 44.7183, Accuracy: 0.5156\n",
      "------Batch 99/274, Batch Loss: 44.5541, Accuracy: 0.5625\n",
      "------Batch 100/274, Batch Loss: 43.8908, Accuracy: 0.5469\n",
      "------Batch 101/274, Batch Loss: 44.4831, Accuracy: 0.5469\n",
      "------Batch 102/274, Batch Loss: 45.0846, Accuracy: 0.5000\n",
      "------Batch 103/274, Batch Loss: 44.9078, Accuracy: 0.5156\n",
      "------Batch 104/274, Batch Loss: 44.7038, Accuracy: 0.5000\n",
      "------Batch 105/274, Batch Loss: 46.9525, Accuracy: 0.3594\n",
      "------Batch 106/274, Batch Loss: 43.8709, Accuracy: 0.5469\n",
      "------Batch 107/274, Batch Loss: 44.0854, Accuracy: 0.5469\n",
      "------Batch 108/274, Batch Loss: 44.2043, Accuracy: 0.5156\n",
      "------Batch 109/274, Batch Loss: 44.3322, Accuracy: 0.5156\n",
      "------Batch 110/274, Batch Loss: 44.6197, Accuracy: 0.4844\n",
      "------Batch 111/274, Batch Loss: 45.1553, Accuracy: 0.4062\n",
      "------Batch 112/274, Batch Loss: 44.3449, Accuracy: 0.5156\n",
      "------Batch 113/274, Batch Loss: 44.8283, Accuracy: 0.4375\n",
      "------Batch 114/274, Batch Loss: 44.2077, Accuracy: 0.3906\n",
      "------Batch 115/274, Batch Loss: 44.7229, Accuracy: 0.5000\n",
      "------Batch 116/274, Batch Loss: 44.0992, Accuracy: 0.4844\n",
      "------Batch 117/274, Batch Loss: 44.3879, Accuracy: 0.5625\n",
      "------Batch 118/274, Batch Loss: 44.1764, Accuracy: 0.5312\n",
      "------Batch 119/274, Batch Loss: 44.8958, Accuracy: 0.4219\n",
      "------Batch 120/274, Batch Loss: 44.2202, Accuracy: 0.5312\n",
      "------Batch 121/274, Batch Loss: 44.7174, Accuracy: 0.5625\n",
      "------Batch 122/274, Batch Loss: 44.3358, Accuracy: 0.5156\n",
      "------Batch 123/274, Batch Loss: 44.3964, Accuracy: 0.5156\n",
      "------Batch 124/274, Batch Loss: 44.0148, Accuracy: 0.5312\n",
      "------Batch 125/274, Batch Loss: 45.0212, Accuracy: 0.4062\n",
      "------Batch 126/274, Batch Loss: 44.9866, Accuracy: 0.4375\n",
      "------Batch 127/274, Batch Loss: 43.9565, Accuracy: 0.6094\n",
      "------Batch 128/274, Batch Loss: 44.9059, Accuracy: 0.4219\n",
      "------Batch 129/274, Batch Loss: 43.8175, Accuracy: 0.5938\n",
      "------Batch 130/274, Batch Loss: 43.7876, Accuracy: 0.6406\n",
      "------Batch 131/274, Batch Loss: 44.2337, Accuracy: 0.5312\n",
      "------Batch 132/274, Batch Loss: 44.3187, Accuracy: 0.5625\n",
      "------Batch 133/274, Batch Loss: 44.6354, Accuracy: 0.4688\n",
      "------Batch 134/274, Batch Loss: 44.3882, Accuracy: 0.5000\n",
      "------Batch 135/274, Batch Loss: 44.7928, Accuracy: 0.4375\n",
      "------Batch 136/274, Batch Loss: 44.3345, Accuracy: 0.5156\n",
      "------Batch 137/274, Batch Loss: 44.5811, Accuracy: 0.5000\n",
      "------Batch 138/274, Batch Loss: 43.7784, Accuracy: 0.6250\n",
      "------Batch 139/274, Batch Loss: 43.9457, Accuracy: 0.5781\n",
      "------Batch 140/274, Batch Loss: 44.1340, Accuracy: 0.5469\n",
      "------Batch 141/274, Batch Loss: 44.6840, Accuracy: 0.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Batch 142/274, Batch Loss: 44.6883, Accuracy: 0.4375\n",
      "------Batch 143/274, Batch Loss: 44.6271, Accuracy: 0.4688\n",
      "------Batch 144/274, Batch Loss: 43.6824, Accuracy: 0.6250\n",
      "------Batch 145/274, Batch Loss: 44.2013, Accuracy: 0.5156\n",
      "------Batch 146/274, Batch Loss: 44.3599, Accuracy: 0.5000\n",
      "------Batch 147/274, Batch Loss: 44.5495, Accuracy: 0.4062\n",
      "------Batch 148/274, Batch Loss: 44.1075, Accuracy: 0.4062\n",
      "------Batch 149/274, Batch Loss: 44.6374, Accuracy: 0.3750\n",
      "------Batch 150/274, Batch Loss: 44.8907, Accuracy: 0.4688\n",
      "------Batch 151/274, Batch Loss: 44.1163, Accuracy: 0.5469\n",
      "------Batch 152/274, Batch Loss: 44.2067, Accuracy: 0.5781\n",
      "------Batch 153/274, Batch Loss: 44.4725, Accuracy: 0.5156\n",
      "------Batch 154/274, Batch Loss: 44.5762, Accuracy: 0.5000\n",
      "------Batch 155/274, Batch Loss: 44.1446, Accuracy: 0.5469\n",
      "------Batch 156/274, Batch Loss: 44.1290, Accuracy: 0.5469\n",
      "------Batch 157/274, Batch Loss: 45.2787, Accuracy: 0.4375\n",
      "------Batch 158/274, Batch Loss: 44.4867, Accuracy: 0.5312\n",
      "------Batch 159/274, Batch Loss: 44.5232, Accuracy: 0.5156\n",
      "------Batch 160/274, Batch Loss: 44.9619, Accuracy: 0.4688\n",
      "------Batch 161/274, Batch Loss: 46.2843, Accuracy: 0.3594\n",
      "------Batch 162/274, Batch Loss: 43.5763, Accuracy: 0.5938\n",
      "------Batch 163/274, Batch Loss: 42.8191, Accuracy: 0.6562\n",
      "------Batch 164/274, Batch Loss: 44.6928, Accuracy: 0.4844\n",
      "------Batch 165/274, Batch Loss: 45.2881, Accuracy: 0.4375\n",
      "------Batch 166/274, Batch Loss: 45.5869, Accuracy: 0.4062\n",
      "------Batch 167/274, Batch Loss: 43.8872, Accuracy: 0.5469\n",
      "------Batch 168/274, Batch Loss: 44.5551, Accuracy: 0.5000\n",
      "------Batch 169/274, Batch Loss: 44.1968, Accuracy: 0.4688\n",
      "------Batch 170/274, Batch Loss: 44.4347, Accuracy: 0.5156\n",
      "------Batch 171/274, Batch Loss: 44.1587, Accuracy: 0.5312\n",
      "------Batch 172/274, Batch Loss: 44.9024, Accuracy: 0.5625\n",
      "------Batch 173/274, Batch Loss: 44.8022, Accuracy: 0.4219\n",
      "------Batch 174/274, Batch Loss: 44.8951, Accuracy: 0.3438\n",
      "------Batch 175/274, Batch Loss: 44.0770, Accuracy: 0.5938\n",
      "------Batch 176/274, Batch Loss: 44.3197, Accuracy: 0.5312\n",
      "------Batch 177/274, Batch Loss: 44.4133, Accuracy: 0.5625\n",
      "------Batch 178/274, Batch Loss: 44.1076, Accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = train(model, train_data, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TRY\n",
    "a) learning rates\n",
    "b) different numbers of hidden layers\n",
    "c) different dim of hidden layers\n",
    "d) try gradient clipping instead of changing the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight updates\n",
    "epoch batch\n",
    "\n",
    "loss&optim function (require grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
